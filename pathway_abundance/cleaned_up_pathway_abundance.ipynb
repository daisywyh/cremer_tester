{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PATHWAY ABUNDANCE MANIPULATION\n",
    "\n",
    "This file focuses only on calculating relative pathway abundance, adding metadata, and cleaning up the data. \n",
    "\n",
    "This pipeline was last edited by Yu Han Daisy Wang on 28 August 2023."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 0: load all relevant/needed packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import font_manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## IF YOU'VE **ALREADY RAN** THIS PIPELINE AND HAVE DATA THAT'S ALREADY CLEANED UP, START HERE\n",
    "Use the following bit to load all of your data that's already been cleaned up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## IF YOU **HAVEN'T ALREADY RAN** THIS PIPELINE/YOU WANT TO CHANGE SOMETHING ABOUT THE DATA PROCESSING, START HERE\n",
    "This starts the data processing from scratch!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load all necessary data for starting from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bowtie hit summaries for propionate\n",
    "compiled_bt_hit_summaries_allpathways = pd.read_csv(\"compiled_bt_hit_summaries_allpathways.csv\")\n",
    "\n",
    "# gene lengths \n",
    "allpathways_gene_catalogue_seqlengths = pd.read_csv(\"allpathways_gene_catalogue_seqlengths.csv\").set_index(\"gene\")\n",
    "\n",
    "# gene information\n",
    "allpathways_gene_info = pd.read_csv(\"allpathways_genesInCatalogue_long.csv\")\n",
    "allpathways_gene_info = allpathways_gene_info.drop(allpathways_gene_info.columns[0], axis=1).set_index(\"strain_gene\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gene length correction (new, using formula proposed by RC, step 1 of it)\n",
    "\n",
    "`gene_length_correction_new` performs the first half of the gene length correction process. Specifically, it does $hits\\;of\\;gene \\cdot length(gene)$. \n",
    "\n",
    "For reference, the full formula for our new gene length correction method is given as follows: $$\\frac{hits\\;of\\;gene \\cdot length(gene)}{hits\\;of\\;rplB \\cdot length(rplB)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gene_length_correction_new(gene_catalogue_seqlengths, compiled_bt_hit_summaries):\n",
    "\n",
    "    gene_length_df = gene_catalogue_seqlengths\n",
    "\n",
    "    df = compiled_bt_hit_summaries.drop([\"pathway\"], axis=1).set_index(\"read_accession\")\n",
    "\n",
    "    new = df.copy()\n",
    "\n",
    "    # first replace the values in new with the gene length of that gene\n",
    "    for gene in compiled_bt_hit_summaries:\n",
    "        \n",
    "        if gene in gene_length_df.index:\n",
    "        \n",
    "            gene_length = gene_length_df.loc[gene].at[\"length\"]\n",
    "\n",
    "            new[gene] = [gene_length] * len(new)\n",
    "\n",
    "    # equivalent to hits of gene * length(gene)\n",
    "    new = df.multiply(new)\n",
    "\n",
    "    return new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### running the code for gene length correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_bt_hit_summaries_all_pathways_length_corrected = gene_length_correction_new(allpathways_gene_catalogue_seqlengths, compiled_bt_hit_summaries_allpathways)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### useful setup for calculating pathway abundances\n",
    "\n",
    "There's not much being done here, just some pure setup stuff that can be useful later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbreviations_dict = {\n",
    "    \"acetylCoA_buk\": \"Ace (buk)\",\n",
    "    \"acetylCoA_but\": \"Ace (but)\",\n",
    "    \"aminobutyrate_buk\": \"4-Ami (buk)\",\n",
    "    \"aminobutyrate_but\": \"4-Ami (but)\",\n",
    "    \"glutarate_buk\": \"Glu (buk)\",\n",
    "    \"glutarate_but\": \"Glu (but)\",\n",
    "    \"lysine\": \"Lys\",\n",
    "    \"sodium-pumping decarboxylase\": \"SP\",\n",
    "    \"Wood-Werkman Cycle\": \"WWC\",\n",
    "    \"acrylate pathway\": \"Acr\",\n",
    "    \"propanediol pathway\": \"Pro\"\n",
    "}\n",
    "\n",
    "pathway_length_dict = {\n",
    "    \"Ace (buk)\": 6,\n",
    "    \"Ace (but)\": 5,\n",
    "    \"4-Ami (buk)\": 6,\n",
    "    \"4-Ami (but)\": 5,\n",
    "    \"Glu (buk)\": 6,\n",
    "    \"Glu (but)\": 5,\n",
    "    \"Lys\": 7,\n",
    "    \"SP\": 4,\n",
    "    \"WWC\": 4,\n",
    "    \"Acr\": 3,\n",
    "    \"Pro\": 5\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculating pathway abundances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `add_genes_in_catalogue` merges information about genes onto the hit table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_genes_in_catalogue(hit_table, gene_info, gene_catalogue_seqlengths):\n",
    "\n",
    "    temp = hit_table.transpose()\n",
    "\n",
    "    # this merges the information about the genes into the hit table\n",
    "    temp = temp.merge(gene_info, how=\"left\", left_index=True, right_index=True)\n",
    "\n",
    "    # everything after this is purely cosmetic, it just moves the columns strain, pathway, \n",
    "    # gene, strain_pathway, and length to the front \n",
    "    strain = temp.pop(\"strain\")\n",
    "    temp.insert(0, strain.name, strain)\n",
    "\n",
    "    pathway = temp.pop(\"pathway\")\n",
    "    temp.insert(1, pathway.name, pathway)\n",
    "\n",
    "    gene = temp.pop(\"gene\")\n",
    "    temp.insert(2, gene.name, gene)\n",
    "\n",
    "    strain_pathway = temp.pop(\"strain_pathway\")\n",
    "    temp.insert(3, strain_pathway.name, strain_pathway)\n",
    "\n",
    "    temp = temp.merge(gene_catalogue_seqlengths, how=\"left\", left_index=True, right_index=True)\n",
    "\n",
    "    length = temp.pop(\"length\")\n",
    "    temp.insert(4, length.name, length)\n",
    "\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_hit_table = add_genes_in_catalogue(compiled_bt_hit_summaries_all_pathways_length_corrected, allpathways_gene_info, allpathways_gene_catalogue_seqlengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### part 2 of length correction: dividing by number of hits for rplB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def length_correction_denominator(hit_table):\n",
    "\n",
    "    grouped = hit_table.groupby(\"strain_pathway\").sum().drop([\"strain\", \"pathway\", \"gene\", \"length\"], axis=1)\n",
    "\n",
    "    rplB_sum = grouped.loc[\"housekeeping\"]\n",
    "\n",
    "    not_samples = [\"strain\", \"pathway\", \"gene\", \"strain_pathway\", \"length\"]\n",
    "\n",
    "    new = hit_table.copy\n",
    "\n",
    "    for read in grouped:\n",
    "\n",
    "        hit_table[read] /= rplB_sum[read]\n",
    "\n",
    "    return hit_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_hit_table = length_correction_denominator(final_hit_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### this step is to normalise by proportion for the unknown samples\n",
    "\n",
    "The formula for this normalisation is given as follows. Let $x$ be the amount of the mixed sample, $S_n$ be the known we are trying to approximate, and $S_i$ be the $i$-th known sample that is in the mixed sample. Let there be $n$ known samples in the mixed sample. As such, the total amount of $S_n$ can be given as follows:\n",
    "\n",
    "$$actual\\;amount\\;of\\;S_n = \\frac{S_n}{\\sum_{i=1}^{n} S_i} \\cdot x$$\n",
    "\n",
    "A function to perform this normalisation is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_unknown(df):\n",
    "\n",
    "    # this function does not operate in place. instead, it returns a modified copy of the original dataframe.\n",
    "    new = df.copy()\n",
    "\n",
    "    # This dictionary stores all of the names of the pathways that are unclear\n",
    "    unknown_samples = {\n",
    "        \"acetylCoA_buk or glutarate_buk\": [\"acetylCoA_buk\", \"glutarate_buk\"],\n",
    "        \"aminobutyrate_but or acetylCoA_but\": [\"aminobutyrate_but\", \"acetylCoA_but\"],\n",
    "        \"acetylCoA_buk or lysine\": [\"acetylCoA_buk\", \"lysine\"],\n",
    "        \"acetylCoA_but or glutarate_but\": [\"acetylCoA_but\", \"glutarate_but\"],\n",
    "        \"acetylCoA_but or glutarate_but or lysine\": [\"acetylCoA_but\", \"glutarate_but\", \"lysine\"],\n",
    "        \"acetylCoA_but or lysine\": [\"acetylCoA_but\", \"lysine\"],\n",
    "        \"aminobutyrate_buk or acetylCoA_buk\": [\"aminobutyrate_buk\", \"acetylCoA_buk\"],\n",
    "        \"aminobutyrate_buk or acetylCoA_buk or lysine\": [\"aminobutyrate_buk\", \"acetylCoA_buk\", \"lysine\"],\n",
    "        \"aminobutyrate_buk or lysine\": [\"aminobutyrate_buk\", \"lysine\"],\n",
    "        \"aminobutyrate_but or acetylCoA_but\": [\"aminobutyrate_but\", \"acetylCoA_but\"],\n",
    "        \"aminobutyrate_but or acetylCoA_but or glutarate_but\": [\"aminobutyrate_but\", \"acetylCoA_but\", \"glutarate_but\"]\n",
    "    }\n",
    "\n",
    "    # for each sample, loop through each pathway in the sample\n",
    "    for sample in df:\n",
    "\n",
    "        for pathway in list(df.index):\n",
    "\n",
    "            # this is my way of finding all the pathways that are ambigious\n",
    "            if \" or \" in pathway:\n",
    "\n",
    "                # this just gets the raw number of hits for the ambigious sample\n",
    "                unknown_proportion = df.loc[pathway].at[sample]\n",
    "\n",
    "                # if the raw number of hits == 0, exit this loop\n",
    "                # if the loop is not exited, this will cause a divide by 0 error later\n",
    "                if unknown_proportion == 0: break\n",
    "\n",
    "                # setup for the rest of the calculations\n",
    "                components_list = unknown_samples[pathway]\n",
    "                proportion_dict = {}\n",
    "                denominator = 0\n",
    "                \n",
    "                # for every component in the unkonwn samples\n",
    "                # if it's != 0, add add it to the denominator\n",
    "                for component in components_list:\n",
    "\n",
    "                    if df.loc[component].at[sample] != 0:\n",
    "\n",
    "                        proportion_dict[component] = df.loc[component].at[sample]\n",
    "                        denominator += proportion_dict[component]\n",
    "\n",
    "                # if there ends up being more than one type of sample in the unknown mixture\n",
    "                # actually do the calculations\n",
    "                # if there's only one, don't bother\n",
    "                if len(proportion_dict) > 1:\n",
    "\n",
    "                    for component in components_list:\n",
    "\n",
    "                        new.loc[component].at[sample] = proportion_dict[component] + ((proportion_dict[component] / denominator) * unknown_proportion)\n",
    "\n",
    "    new = new.drop(list(unknown_samples.keys()))\n",
    "\n",
    "    return new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, actually implement the `normalisation_unknown` function on actual data\n",
    "\n",
    "First, perform some preprocessing of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_pathway_group = final_hit_table.groupby([\"strain_pathway\"]).sum()\n",
    "\n",
    "overall_pathway_group = overall_pathway_group.drop([\"strain\", \"pathway\", \"gene\", \"length\"], axis=1).drop([\"housekeeping\"], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, actually implement everything. The code for parsing out the unknown proportions is not very well written, so this section might take a long time to run, as long as just under 10 minutes. For context, the last time I tried to run this chunk, it took me 7 mins and 31 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalised_unknown_overall_pathway_group = normalise_unknown(overall_pathway_group)\n",
    "\n",
    "# the next two steps are purely for aesthetic reasons\n",
    "# it just orders it so that the propionate and butyrate pathways are together\n",
    "normalised_unknown_overall_pathway_group = normalised_unknown_overall_pathway_group.transpose()\n",
    "normalised_unknown_overall_pathway_group = normalised_unknown_overall_pathway_group.reindex(columns= [\"acetylCoA_buk\", \"acetylCoA_but\", \"aminobutyrate_buk\", \"aminobutyrate_but\", \"glutarate_buk\", \"glutarate_but\", \"lysine\", \"sodium-pumping decarboxylase\", \"Wood-Werkman Cycle\", \"acrylate pathway\", \"propanediol pathway\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fixing column names to the abbreviations\n",
    "\n",
    "this next step just changes things from the raw names to the abbreviations established earlier. It does this by looping through the `abbreviations_dict` established earlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in normalised_unknown_overall_pathway_group:\n",
    "    normalised_unknown_overall_pathway_group.rename(columns = {column:abbreviations_dict[column]}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### normalisation with respect to the length of each pathway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pathway in normalised_unknown_overall_pathway_group:\n",
    "\n",
    "    # if the pathway is in our pathway_length_dict (it should be, this step is just to make sure)\n",
    "    # then divide the abundance by the length of that pathway\n",
    "    if pathway in pathway_length_dict.keys():\n",
    "\n",
    "        normalised_unknown_overall_pathway_group[pathway] /= pathway_length_dict[pathway]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now add what the actual final pathways are\n",
    "\n",
    "This part simply goes through the entire file/column to figure out what pathway produces, then appends this result to the given table.\n",
    "\n",
    "The input table should have the **pathways** as the index, and the **samples** as the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pathway(df):\n",
    "\n",
    "    new = df.copy()\n",
    "\n",
    "    prop_pathways = [\"SP\", \"WWC\", \"Pro\", \"Acr\"]\n",
    "\n",
    "    butyrate_pathways = [\"Ace (buk)\", \"4-Ami (buk)\", \"Lys\", \"Glu (but)\", \"Glu (buk)\", \"Ace (but)\", \"4-Ami (but)\"]\n",
    "\n",
    "    pathway_result = []\n",
    "\n",
    "    # for each pathway in \n",
    "    for path in list(new.index):\n",
    "\n",
    "        if path in prop_pathways: pathway_result.append(\"PROP\")\n",
    "\n",
    "        if path in butyrate_pathways: pathway_result.append(\"BTR\")\n",
    "\n",
    "    new.insert(0, \"overall_pathway\", pathway_result)\n",
    "\n",
    "    return new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the function `find_pathway` to the table `normalised_unknown_overall_pathway_group`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_but_groupby = find_pathway(normalised_unknown_overall_pathway_group.transpose()).groupby([\"overall_pathway\"]).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add the stuff about metadata + species \n",
    "\n",
    "This code is taken directly from August Burton, who I believe might have first gotten this code from Rebecca Christesen (RC)? Either way, all this code does is get you one final table that you need to care about (`relab2`), which contains taxonomical information for what I believe to be most, if not all, samples from the curated metagenomics dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2620588/2769290257.py:8: DtypeWarning: Columns (21,22,27,29,30,33,36,43,50,52,53,54,55,56,57,62,63,70,72,76,77,87,90,98,99,102,106,107,108,109,111,114,115,116,123) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  colnames=pd.read_csv(\"data_curated_microbiome/relabundance_colData.csv\")\n"
     ]
    }
   ],
   "source": [
    "# Load curated microbiome data\n",
    "# The original data is provided in R. Here, we use only the abundance data, provided in easily to handle csv files (subfolder data_curated_microbiome). The abundance levels were extracted from metagenomics using https://github.com/biobakery/MetaPhlAn.\n",
    "#big table with abundance of different species across all thousands of samples from the data collection\n",
    "relab=pd.read_csv(\"data_curated_microbiome/relabundance.csv\")\n",
    "relab.rename(columns={'Unnamed: 0': 'tax_identifier'},inplace=True) # Gives first column label \"tax_identifier\"\n",
    "\n",
    "#information about samples\n",
    "colnames=pd.read_csv(\"data_curated_microbiome/relabundance_colData.csv\")\n",
    "colnames.rename(columns={'Unnamed: 0': 'sample'},inplace=True) # Gives name to first column\n",
    "\n",
    "#information about different species detected in the different samples\n",
    "rownames=pd.read_csv(\"data_curated_microbiome/relabundance_rowData.csv\")\n",
    "rownames.rename(columns={'Unnamed: 0': 'tax_identifier'},inplace=True)\n",
    "\n",
    "#add species information to major data table. Used for groupby analysis later on\n",
    "relab2 = relab.merge(rownames, on='tax_identifier', how='inner')  # Rows = species, Cols = Samples + species info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding metadata into it all\n",
    "\n",
    "This adds metadata to both the versions where we've grouped by prop vs btr already (`with_metadata_prop_but_normalised`), or we're still at the pathway level (`with_metadata_overall_group_normalised`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv(\"data_curated_microbiome/nayfach_asnicar_hmp_metadata.csv\").set_index(\"NCBI_accession\")\n",
    "\n",
    "temp = prop_but_groupby.transpose()\n",
    "\n",
    "with_metadata_prop_but_normalised = temp.merge(metadata, how=\"left\", left_index=True, right_index=True)\n",
    "\n",
    "with_metadata_prop_but_normalised.to_csv(\"output/with_metadata_prop_but_normalised.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = normalised_unknown_overall_pathway_group\n",
    "\n",
    "with_metadata_overall_group_normalised = temp.merge(metadata, how=\"left\", left_index=True, right_index=True)\n",
    "\n",
    "with_metadata_overall_group_normalised.to_csv(\"output/with_metadata_overall_group_normalised.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **RELOAD DATA HERE! RELOAD THE FIRST VERSION OF DATA METADATA!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_metadata_prop_but_normalised = pd.read_csv(\"output/with_metadata_prop_but_normalised.csv\").set_index(\"Unnamed: 0.1\")\n",
    "\n",
    "with_metadata_overall_group_normalised = pd.read_csv(\"output/with_metadata_overall_group_normalised\").set_index(\"Unnamed: 0.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now lets normalise for percentage\n",
    "\n",
    "Both of these functions simply convert everything from regular stuff to percentages.\n",
    "\n",
    "`percentageNormaliseOne` does normalisation correctly if there is no metadata attached. It requires the given data to have the **pathways** as the index, and the **samples** as the columns.\n",
    "\n",
    "`percentageNormaliseTwo` does normalisation correctly only if there is metadata attached. It requires teh given data to have the **samples** as the index, and **pathways** as the columns. It also accounts for the case of dividing by 0, in which it just fills in `np.nan`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentageNormaliseOne(df):\n",
    "\n",
    "    new = df.copy()\n",
    "\n",
    "    summed = df.sum(axis=0)\n",
    "\n",
    "    for row in df.index:\n",
    "\n",
    "        new.loc[row] = df.loc[row].div(summed)\n",
    "\n",
    "    return new\n",
    "\n",
    "def percentageNormaliseTwo(df, pathways):\n",
    "\n",
    "    summed = df[pathways[0]].add(df[pathways[1]])\n",
    "\n",
    "    for pathway in pathways:\n",
    "\n",
    "        pathway_name = pathway+\"_percent\"\n",
    "        \n",
    "        df[pathway_name] = df[pathway].div(summed)\n",
    "\n",
    "        df.loc[~np.isfinite(df[pathway_name]), pathway_name] = np.nan\n",
    "\n",
    "    return df.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Percentage normalisation in actiion\n",
    "\n",
    "The following applies the percentage normalisation function to all of the actual data. \n",
    "\n",
    "Note: here, I included these two metrics: `BTR % - PROP %` and `BTR - PROP`. I highly advise against using these metrics, as I don't think that they add anything of value to our analysis. The metrics are as follows:\n",
    "- `BTR % - PROP %` is the percentage of BTR pathways minus the percentage of PROP pathways. \n",
    "- `BTR - PROP` is the raw relative abundance of BTR pathways minus the percentage of PROP pathways\n",
    "\n",
    "Otherwise, the percentages are listed under "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_metadata_prop_but_percent_change = percentageNormaliseTwo(with_metadata_prop_but_normalised, [\"BTR\", \"PROP\"])\n",
    "\n",
    "with_metadata_prop_but_percent_change.loc[\"BTR % - PROP %\"] = with_metadata_prop_but_percent_change.loc[\"BTR_percent\"] - with_metadata_prop_but_percent_change.loc[\"PROP_percent\"]\n",
    "\n",
    "with_metadata_prop_but_percent_change.loc[\"BTR - PROP\"] = with_metadata_prop_but_percent_change.loc[\"BTR\"] - with_metadata_prop_but_percent_change.loc[\"PROP\"]\n",
    "\n",
    "with_metadata_prop_but_percent_change = with_metadata_prop_but_percent_change.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This adds my own binning for age. I felt like the way that binning was done for the data wasn't very useful, so I redid the binning according to NIH style guidelines (https://www.nih.gov/nih-style-guide/age)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_category_list = []\n",
    "\n",
    "index = 0\n",
    "\n",
    "# go through every sample to get their age, and bin it accordingly\n",
    "for sample in with_metadata_prop_but_percent_change.age:\n",
    "\n",
    "    if sample <=1:\n",
    "        if with_metadata_prop_but_percent_change.infant_age[index] <= 30:\n",
    "            age_category_list.append(\"Newborn\")\n",
    "        else:\n",
    "            age_category_list.append(\"Infant\")\n",
    "\n",
    "    elif sample > 1 and sample <=12: age_category_list.append(\"Child\")\n",
    "\n",
    "    elif sample > 12 and sample < 18: age_category_list.append(\"Adolescent\")\n",
    "\n",
    "    elif sample >= 18 and sample < 65: age_category_list.append(\"Adult\")\n",
    "\n",
    "    elif sample >= 65: age_category_list.append(\"Older Adult\")\n",
    "\n",
    "    else: age_category_list.append(np.nan)\n",
    "\n",
    "    index += 1\n",
    "\n",
    "# update the age_category column with the correct values\n",
    "with_metadata_prop_but_percent_change[\"age_category\"] = age_category_list\n",
    "\n",
    "# fix the \"infant_age\" column name to be \"Infant Age (days)\" instead\n",
    "with_metadata_prop_but_percent_change = with_metadata_prop_but_percent_change.rename(columns={\"infant_age\": \"Infant Age (days)\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now save everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_metadata_prop_but_percent_change.to_csv(\"output/with_metadata_prop_but_percent_change.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RELOAD THE DATA HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_metadata_prop_but_percent_change = pd.read_csv(\"output/with_metadata_prop_but_percent_change.csv\").set_index(\"Unnamed: 0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "healthy_overall_group_noramlised = with_metadata_overall_group_normalised.loc[with_metadata_overall_group_normalised['disease'] == \"healthy\"]\n",
    "\n",
    "healthy_prop_but_noramlised = with_metadata_prop_but_percent_change.loc[with_metadata_prop_but_percent_change['disease'] == \"healthy\"]\n",
    "\n",
    "adult_healthy_overall_group_normalised = healthy_overall_group_noramlised.loc[healthy_overall_group_noramlised[\"age\"] >= 18]\n",
    "\n",
    "adult_health_prop_but_normalised = with_metadata_prop_but_percent_change.loc[with_metadata_prop_but_percent_change[\"age\"] >= 18]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add the taxonomical information in\n",
    "\n",
    "This code is taken directly from August Burton, who I believe might have first gotten this code from Rebecca Christesen (RC)? Either way, all this code does is get you one final table that you need to care about (`relab2`), which contains taxonomical information for what I believe to be most, if not all, samples from the curated metagenomics dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2620588/2769290257.py:8: DtypeWarning: Columns (21,22,27,29,30,33,36,43,50,52,53,54,55,56,57,62,63,70,72,76,77,87,90,98,99,102,106,107,108,109,111,114,115,116,123) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  colnames=pd.read_csv(\"data_curated_microbiome/relabundance_colData.csv\")\n"
     ]
    }
   ],
   "source": [
    "# Load curated microbiome data\n",
    "# The original data is provided in R. Here, we use only the abundance data, provided in easily to handle csv files (subfolder data_curated_microbiome). The abundance levels were extracted from metagenomics using https://github.com/biobakery/MetaPhlAn.\n",
    "#big table with abundance of different species across all thousands of samples from the data collection\n",
    "relab=pd.read_csv(\"data_curated_microbiome/relabundance.csv\")\n",
    "relab.rename(columns={'Unnamed: 0': 'tax_identifier'},inplace=True) # Gives first column label \"tax_identifier\"\n",
    "\n",
    "#information about samples\n",
    "colnames=pd.read_csv(\"data_curated_microbiome/relabundance_colData.csv\")\n",
    "colnames.rename(columns={'Unnamed: 0': 'sample'},inplace=True) # Gives name to first column\n",
    "\n",
    "#information about different species detected in the different samples\n",
    "rownames=pd.read_csv(\"data_curated_microbiome/relabundance_rowData.csv\")\n",
    "rownames.rename(columns={'Unnamed: 0': 'tax_identifier'},inplace=True)\n",
    "\n",
    "#add species information to major data table. Used for groupby analysis later on\n",
    "relab2 = relab.merge(rownames, on='tax_identifier', how='inner')  # Rows = species, Cols = Samples + species info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this takes only the phylum level data, as currently that's what we care the most about\n",
    "# however, please feel free to modify this as needed/as you would like\n",
    "phylum_df = relab2.groupby(\"phylum\").sum().drop([\"superkingdom\", \"class\", \"order\", \"family\", \"genus\", \"species\"], axis=1).reset_index().set_index(\"phylum\").drop([\"tax_identifier\"], axis=1)\n",
    "\n",
    "temp = phylum_df.transpose()\n",
    "\n",
    "metadata = pd.read_csv(\"data_curated_microbiome/nayfach_asnicar_hmp_metadata.csv\").drop([\"infant_age\", \"age_category\", \"gender\", \"country\", \"non_westernized\", \"BMI\"], axis=1).set_index(\"Unnamed: 0\")\n",
    "\n",
    "with_phylum = metadata.merge(temp, how=\"left\", left_index=True, right_index=True).reset_index().set_index(\"NCBI_accession\")\n",
    "\n",
    "with_phylum.to_csv(\"output/with_phylum.csv\")\n",
    "\n",
    "# get only Bacteroidetes and Firmicutes\n",
    "bacteroidetes_vs_firmicutes = with_phylum[[\"Bacteroidetes\", \"Firmicutes\"]]\n",
    "\n",
    "bacteroidetes_vs_firmicutes = bacteroidetes_vs_firmicutes.reindex([\"Firmicutes\", \"Bacteroidetes\"], axis=\"columns\").transpose()\n",
    "\n",
    "adult_healthy_bacteroidetes_vs_firmicutes = with_phylum[[\"Bacteroidetes\", \"Firmicutes\", \"age\", \"disease\"]]\n",
    "\n",
    "adult_healthy_bacteroidetes_vs_firmicutes = adult_healthy_bacteroidetes_vs_firmicutes.loc[adult_healthy_bacteroidetes_vs_firmicutes[\"age\"] >= 18]\n",
    "\n",
    "adult_healthy_bacteroidetes_vs_firmicutes = adult_healthy_bacteroidetes_vs_firmicutes.loc[adult_healthy_bacteroidetes_vs_firmicutes[\"disease\"] == \"healthy\"]\n",
    "\n",
    "adult_healthy_bacteroidetes_vs_firmicutes = adult_healthy_bacteroidetes_vs_firmicutes.drop([\"age\", \"disease\"], axis=1).reindex([\"Firmicutes\", \"Bacteroidetes\"], axis=\"columns\").transpose()\n",
    "\n",
    "adult_healthy_bacteroidetes_vs_firmicutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_samples = adult_healthy_bacteroidetes_vs_firmicutes.transpose().reset_index().dropna()\n",
    "\n",
    "valid_samples = valid_samples.set_index(\"NCBI_accession\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = adult_health_prop_but_normalised[[\"BTR\", \"PROP\"]]\n",
    "\n",
    "adult_health_prop_but_percent = percentageNormaliseOne(temp.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_merged = adult_health_prop_but_percent.transpose().merge(valid_samples, how=\"left\", left_index=True, right_index=True)\n",
    "\n",
    "temp_merged = percentageNormaliseTwo(temp_merged, [\"Firmicutes\", \"Bacteroidetes\"])\n",
    "\n",
    "temp_merged = temp_merged.transpose().sort_values(\"Firmicutes_percent\")\n",
    "\n",
    "sorted_adult_healthy_bacteroidetes_vs_firmicutes_percent = temp_merged[[\"Firmicutes_percent\", \"Bacteroidetes_percent\"]].transpose()\n",
    "\n",
    "sorted_adult_healthy_prop_but_percent = temp_merged[[\"BTR\", \"PROP\"]].transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## random plots testing time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### custom fonts because why not I'm annoying like that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "font_dirs = [\"Hind\"]\n",
    "font_files = font_manager.findSystemFonts(fontpaths=font_dirs)\n",
    "\n",
    "for font_file in font_files:\n",
    "    font_manager.fontManager.addfont(font_file)\n",
    "\n",
    "# set font\n",
    "plt.rcParams['font.family'] = \"Hind\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### defining functions to make some basic stacked bar plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stackedBarDF(overall_pathway_df):\n",
    "\n",
    "    temp_table = overall_pathway_df.transpose()\n",
    "\n",
    "    temp_dict = {}\n",
    "\n",
    "    for column in temp_table:\n",
    "\n",
    "        temp_dict[column] = temp_table[column].tolist()\n",
    "\n",
    "    samples_list = list(temp_table.index.values)\n",
    "\n",
    "    plottingDF = pd.DataFrame(\n",
    "        temp_dict,\n",
    "        index = samples_list\n",
    "    )\n",
    "\n",
    "    return plottingDF.fillna(0) \n",
    "\n",
    "def plotStackedBar(stackedBarDF, name=\"overall pathway groupby\", x_name=\"samples\"):\n",
    "\n",
    "    mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "    n = len(stackedBarDF.columns)\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, n))\n",
    "\n",
    "    plt.rcParams.update({'font.size': 33})\n",
    "    plt.tight_layout()\n",
    "    plt.rcParams['figure.dpi']=600\n",
    "    plt.rcParams['font.family'] = \"Hind\"\n",
    "    \n",
    "    return stackedBarDF.plot(kind=\"bar\", stacked=True, color=colors, figsize=(25,7), xlabel=x_name, ylabel=\"relative pathway abundance\", title=name, xticks=([])).legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "    \n",
    "def sortedStackedBar(stackedBarDF, sortBy, auto_name=False, x_name=\"samples\"):\n",
    "\n",
    "    # stackedBarDF = stackedBarDF[[\"butyrate_percent\", \"propionate_percent\"]] if percent else stackedBarDF[[\"BTR\", \"PROP\"]]\n",
    "    \n",
    "    name = \"Sorted by abundance of \" + sortBy if not auto_name else \"\"\n",
    "\n",
    "    return plotStackedBar(stackedBarDF.sort_values(by=sortBy), name, x_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### defining stuff for scatter plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotScatter(scatterDF, x_data, y_data):\n",
    "\n",
    "    plt.rcParams.update({'font.size': 23})\n",
    "    plt.tight_layout()\n",
    "    plt.rcParams['figure.dpi']=600\n",
    "    sns.set_style(\"darkgrid\")\n",
    "\n",
    "    name = \"Proportions of \" + x_data + \" producers versus\\n\" + y_data + \" producers for 1203 healthy adult samples\"\n",
    "    filename = x_data + \"_vs_\" + y_data\n",
    "    \n",
    "    fig = scatterDF.plot(kind=\"scatter\", x=x_data, y=y_data, figsize=(15,6),colormap=\"viridis\", xlabel=\"relative abundance of \" + x_data, ylabel=\"relative abundance of \" + y_data, title=name)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def plotScatterContinuous(scatterDF, x_data, y_data, c_data):\n",
    "\n",
    "    scatterDF = scatterDF.dropna(subset=[c_data], axis=0)\n",
    "\n",
    "    plt.rcParams.update({'font.size': 20})\n",
    "    plt.tight_layout()\n",
    "    plt.rcParams['figure.dpi']=600\n",
    "    sns.set_style(\"darkgrid\")\n",
    "\n",
    "    name = x_data + \" vs \" + y_data + \" with respect to \" + c_data\n",
    "    \n",
    "    fig = scatterDF.plot(kind=\"scatter\", x=x_data, y=y_data, c=c_data, colormap=\"viridis\", alpha=0.7, figsize=(15,5), xlabel=\"relative abundance of \" + x_data, ylabel=\"relative abundance of \" + y_data, title=name)\n",
    "\n",
    "def plotScatterDiscrete(scatterDF, x_data, y_data, c_data):\n",
    "\n",
    "    name = x_data + \" vs \" + y_data + \" with respect to \" + c_data\n",
    "\n",
    "    sns.reset_defaults()\n",
    "    sns.set_style(\"darkgrid\")\n",
    "    plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "    plt.rcParams['figure.dpi']=600\n",
    "    plt.figure(figsize=(14, 5))\n",
    "\n",
    "    sns.set_palette(wesanderson.film_palette(\"darjeeling\"))\n",
    "\n",
    "    sns.scatterplot(data=scatterDF,x=x_data, y=y_data, hue=c_data, alpha=0.5).set(xlabel=\"relative abundance of \" + x_data, ylabel=\"relative abundance of \" + y_data, title=name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### defining stuff for line/scatter plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotScatterLine(df, x_data, y_data, name):\n",
    "    \n",
    "    sns.reset_defaults()\n",
    "    sns.set_style(\"darkgrid\")\n",
    "    plt.rcParams.update({'font.size': 53})\n",
    "    plt.rcParams['font.family'] = \"Hind\"\n",
    "    plt.rcParams['figure.dpi']=600\n",
    "    sns.set_palette(\"viridis\")\n",
    "    plt.figure(figsize=(31, 10))\n",
    "    \n",
    "    return sns.regplot(data=df, x=x_data, y=y_data, x_jitter=.05, lowess=True, scatter_kws={\"s\": 500, 'alpha': 0.3}).set(title=name, ylabel=\"Relative Abundance of BTR\")\n",
    "\n",
    "def plotStrip(df, x_data, y_data, c_data, order_list, name):\n",
    "    \n",
    "    sns.reset_defaults()\n",
    "    sns.set_style(\"darkgrid\")\n",
    "    plt.rcParams.update({'font.size': 53})\n",
    "    plt.rcParams['font.family'] = \"Hind\"\n",
    "    plt.rcParams['figure.dpi']=600\n",
    "    sns.set_palette(\"viridis\")\n",
    "    plt.figure(figsize=(31, 10))\n",
    "    \n",
    "    temp = df.reset_index()\n",
    "\n",
    "    sns.set_palette(\"viridis\")\n",
    "\n",
    "    sns.stripplot(data=temp, x=x_data, y=y_data, hue=c_data, order=order_list, dodge=True, alpha=0.4, s=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### defining stuff for violin plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def violinDF(df, category):\n",
    "\n",
    "    temp = pd.melt(df.reset_index(), id_vars=\"Unnamed: 0.1\", value_vars=[\"butyrate\", \"propionate\"])\n",
    "\n",
    "    temp = temp.set_index(\"Unnamed: 0.1\").rename(columns={\"value\": \"Relative Abundance\"})\n",
    "    \n",
    "    temp1 = df[[category]]\n",
    "\n",
    "    return temp.merge(temp1, how=\"left\", left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "def plotViolin(df, x_data, y_data, c_data, order_list, split_option, name):\n",
    "    \n",
    "    sns.reset_defaults()\n",
    "    sns.set_style(\"darkgrid\")\n",
    "    plt.rcParams.update({'font.size': 53})\n",
    "    plt.rcParams['font.family'] = \"Hind\"\n",
    "    plt.rcParams['figure.dpi']=600\n",
    "    plt.figure(figsize=(31, 10))\n",
    "    sns.set_palette(\"viridis_r\")\n",
    "\n",
    "    sns.violinplot(data=df, x=x_data, y=y_data, hue=c_data, order=order_list, cut=0, inner=\"stick\", split=split_option, bw=0.2).set(title=name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4 ('pathway_abundance')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fe830e55702ed5d1a418e856f39da784e7da7ed40fb81b27836f6ec2261e36fe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
